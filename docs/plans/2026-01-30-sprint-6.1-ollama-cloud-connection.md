# Sprint 6.1: Ollama Cloud Connection Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Create a robust Ollama Cloud client for LLM text generation with retry logic, rate limit handling, and health check endpoint.

**Architecture:** Separate client module for Ollama Cloud (LLM generation) distinct from existing local Ollama (embeddings). Client handles authentication, retries with exponential backoff, and provides structured error handling. Health check endpoint allows admins to verify connection status.

**Tech Stack:** TypeScript, Next.js API routes, pino logger, fetch API

**Note:** Ollama API quota is currently maxed out - setup can be completed but live API calls won't work until quota resets in ~2 days.

---

## Task 1: Add AI Logger

**Files:**
- Modify: `apps/admin/lib/logger.ts`

**Step 1: Add AI logger export**

Add after the existing logger exports in `apps/admin/lib/logger.ts`:

```typescript
export const aiLogger = logger.child({ module: 'ai' });
```

**Step 2: Verify**

Run: `pnpm type-check`
Expected: PASS

**Step 3: Commit**

```bash
git add apps/admin/lib/logger.ts
git commit -m "feat(ai): add AI module logger"
```

---

## Task 2: Create AI Types

**Files:**
- Create: `apps/admin/lib/ai/types.ts`

**Step 1: Create types file**

Create `apps/admin/lib/ai/types.ts`:

```typescript
/**
 * AI service types for Ollama Cloud and local Ollama
 */

// =============================================================================
// Ollama Cloud Types (LLM Generation)
// =============================================================================

export interface OllamaCloudConfig {
  apiKey: string;
  baseUrl: string;
  model: string;
  maxRetries: number;
  retryDelayMs: number;
}

export interface OllamaGenerateRequest {
  model: string;
  prompt: string;
  system?: string;
  stream?: boolean;
  options?: {
    temperature?: number;
    top_p?: number;
    max_tokens?: number;
  };
}

export interface OllamaGenerateResponse {
  model: string;
  created_at: string;
  response: string;
  done: boolean;
  context?: number[];
  total_duration?: number;
  load_duration?: number;
  prompt_eval_count?: number;
  prompt_eval_duration?: number;
  eval_count?: number;
  eval_duration?: number;
}

export interface OllamaModelInfo {
  name: string;
  modified_at: string;
  size: number;
}

export interface OllamaModelsResponse {
  models: OllamaModelInfo[];
}

// =============================================================================
// AI Client Result Types
// =============================================================================

export interface AiResult<T> {
  success: true;
  data: T;
}

export interface AiError {
  success: false;
  error: {
    code: 'RATE_LIMITED' | 'AUTH_ERROR' | 'MODEL_NOT_FOUND' | 'NETWORK_ERROR' | 'TIMEOUT' | 'UNKNOWN';
    message: string;
    retryAfter?: number;
  };
}

export type AiResponse<T> = AiResult<T> | AiError;

// =============================================================================
// Health Check Types
// =============================================================================

export interface OllamaHealthStatus {
  connected: boolean;
  model: string;
  modelAvailable: boolean;
  latencyMs: number | null;
  error: string | null;
  checkedAt: string;
}
```

**Step 2: Verify**

Run: `pnpm type-check`
Expected: PASS

**Step 3: Commit**

```bash
git add apps/admin/lib/ai/types.ts
git commit -m "feat(ai): add AI service types"
```

---

## Task 3: Create Ollama Cloud Client

**Files:**
- Create: `apps/admin/lib/ai/ollama-cloud.ts`

**Step 1: Create Ollama Cloud client**

Create `apps/admin/lib/ai/ollama-cloud.ts`:

```typescript
/**
 * Ollama Cloud client for LLM text generation
 * Handles authentication, retries, and rate limiting
 */

import { aiLogger } from '../logger';

import type {
  AiResponse,
  OllamaCloudConfig,
  OllamaGenerateRequest,
  OllamaGenerateResponse,
  OllamaHealthStatus,
  OllamaModelsResponse,
} from './types';

// =============================================================================
// Configuration
// =============================================================================

const DEFAULT_CONFIG: Omit<OllamaCloudConfig, 'apiKey'> = {
  baseUrl: 'https://api.ollama.com',
  model: 'llama3.1:70b',
  maxRetries: 3,
  retryDelayMs: 30000, // 30 seconds base delay
};

function getConfig(): OllamaCloudConfig {
  const apiKey = process.env.OLLAMA_CLOUD_API_KEY;

  if (!apiKey) {
    throw new Error('OLLAMA_CLOUD_API_KEY environment variable is not set');
  }

  return {
    apiKey,
    baseUrl: process.env.OLLAMA_CLOUD_URL || DEFAULT_CONFIG.baseUrl,
    model: process.env.OLLAMA_CLOUD_MODEL || DEFAULT_CONFIG.model,
    maxRetries: DEFAULT_CONFIG.maxRetries,
    retryDelayMs: DEFAULT_CONFIG.retryDelayMs,
  };
}

// =============================================================================
// HTTP Helpers
// =============================================================================

async function fetchWithAuth(
  url: string,
  options: RequestInit = {}
): Promise<Response> {
  const config = getConfig();

  return fetch(url, {
    ...options,
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${config.apiKey}`,
      ...options.headers,
    },
  });
}

function sleep(ms: number): Promise<void> {
  return new Promise(resolve => setTimeout(resolve, ms));
}

// =============================================================================
// Retry Logic
// =============================================================================

async function withRetry<T>(
  operation: () => Promise<AiResponse<T>>,
  context: string
): Promise<AiResponse<T>> {
  const config = getConfig();
  let lastError: AiResponse<T> | null = null;

  for (let attempt = 1; attempt <= config.maxRetries; attempt++) {
    const result = await operation();

    if (result.success) {
      return result;
    }

    lastError = result;

    // Don't retry on auth errors or model not found
    if (result.error.code === 'AUTH_ERROR' || result.error.code === 'MODEL_NOT_FOUND') {
      aiLogger.error(
        { code: result.error.code, context },
        `Non-retryable error: ${result.error.message}`
      );
      return result;
    }

    // Calculate exponential backoff delay
    const delay = config.retryDelayMs * Math.pow(2, attempt - 1);

    aiLogger.warn(
      { attempt, maxRetries: config.maxRetries, delay, context, code: result.error.code },
      `Retrying after ${delay}ms (attempt ${attempt}/${config.maxRetries})`
    );

    if (attempt < config.maxRetries) {
      await sleep(delay);
    }
  }

  aiLogger.error(
    { context, attempts: config.maxRetries },
    `All retry attempts exhausted`
  );

  return lastError!;
}

// =============================================================================
// API Methods
// =============================================================================

/**
 * Check if Ollama Cloud is configured
 */
export function isOllamaCloudConfigured(): boolean {
  return !!process.env.OLLAMA_CLOUD_API_KEY;
}

/**
 * Get the configured model name
 */
export function getConfiguredModel(): string {
  return process.env.OLLAMA_CLOUD_MODEL || DEFAULT_CONFIG.model;
}

/**
 * List available models
 */
export async function listModels(): Promise<AiResponse<OllamaModelsResponse>> {
  const config = getConfig();

  try {
    const response = await fetchWithAuth(`${config.baseUrl}/api/tags`);

    if (response.status === 401 || response.status === 403) {
      return {
        success: false,
        error: {
          code: 'AUTH_ERROR',
          message: 'Invalid or expired API key',
        },
      };
    }

    if (response.status === 429) {
      const retryAfter = parseInt(response.headers.get('Retry-After') || '60', 10);
      return {
        success: false,
        error: {
          code: 'RATE_LIMITED',
          message: 'Rate limit exceeded',
          retryAfter,
        },
      };
    }

    if (!response.ok) {
      return {
        success: false,
        error: {
          code: 'UNKNOWN',
          message: `API error: ${response.status} ${response.statusText}`,
        },
      };
    }

    const data = await response.json() as OllamaModelsResponse;
    return { success: true, data };
  } catch (error) {
    aiLogger.error({ error }, 'Failed to list models');
    return {
      success: false,
      error: {
        code: 'NETWORK_ERROR',
        message: error instanceof Error ? error.message : 'Network error',
      },
    };
  }
}

/**
 * Generate text using Ollama Cloud LLM
 * Includes automatic retry with exponential backoff
 */
export async function generate(
  prompt: string,
  options: {
    system?: string;
    temperature?: number;
    maxTokens?: number;
  } = {}
): Promise<AiResponse<OllamaGenerateResponse>> {
  const config = getConfig();

  const operation = async (): Promise<AiResponse<OllamaGenerateResponse>> => {
    try {
      const request: OllamaGenerateRequest = {
        model: config.model,
        prompt,
        stream: false,
        system: options.system,
        options: {
          temperature: options.temperature ?? 0.7,
          max_tokens: options.maxTokens ?? 2048,
        },
      };

      const response = await fetchWithAuth(`${config.baseUrl}/api/generate`, {
        method: 'POST',
        body: JSON.stringify(request),
      });

      if (response.status === 401 || response.status === 403) {
        return {
          success: false,
          error: {
            code: 'AUTH_ERROR',
            message: 'Invalid or expired API key',
          },
        };
      }

      if (response.status === 429) {
        const retryAfter = parseInt(response.headers.get('Retry-After') || '60', 10);
        return {
          success: false,
          error: {
            code: 'RATE_LIMITED',
            message: 'Rate limit exceeded',
            retryAfter,
          },
        };
      }

      if (response.status === 404) {
        return {
          success: false,
          error: {
            code: 'MODEL_NOT_FOUND',
            message: `Model ${config.model} not found`,
          },
        };
      }

      if (!response.ok) {
        return {
          success: false,
          error: {
            code: 'UNKNOWN',
            message: `API error: ${response.status} ${response.statusText}`,
          },
        };
      }

      const data = await response.json() as OllamaGenerateResponse;

      aiLogger.info(
        {
          model: data.model,
          promptTokens: data.prompt_eval_count,
          completionTokens: data.eval_count,
          durationMs: data.total_duration ? data.total_duration / 1_000_000 : null,
        },
        'Generation completed'
      );

      return { success: true, data };
    } catch (error) {
      if (error instanceof Error && error.name === 'AbortError') {
        return {
          success: false,
          error: {
            code: 'TIMEOUT',
            message: 'Request timed out',
          },
        };
      }

      aiLogger.error({ error }, 'Generation request failed');
      return {
        success: false,
        error: {
          code: 'NETWORK_ERROR',
          message: error instanceof Error ? error.message : 'Network error',
        },
      };
    }
  };

  return withRetry(operation, 'generate');
}

/**
 * Check Ollama Cloud health and connectivity
 */
export async function checkHealth(): Promise<OllamaHealthStatus> {
  const checkedAt = new Date().toISOString();
  const model = getConfiguredModel();

  if (!isOllamaCloudConfigured()) {
    return {
      connected: false,
      model,
      modelAvailable: false,
      latencyMs: null,
      error: 'OLLAMA_CLOUD_API_KEY not configured',
      checkedAt,
    };
  }

  const startTime = Date.now();

  try {
    const result = await listModels();
    const latencyMs = Date.now() - startTime;

    if (!result.success) {
      return {
        connected: false,
        model,
        modelAvailable: false,
        latencyMs,
        error: result.error.message,
        checkedAt,
      };
    }

    const modelAvailable = result.data.models.some(
      m => m.name === model || m.name.startsWith(model.split(':')[0])
    );

    return {
      connected: true,
      model,
      modelAvailable,
      latencyMs,
      error: null,
      checkedAt,
    };
  } catch (error) {
    return {
      connected: false,
      model,
      modelAvailable: false,
      latencyMs: Date.now() - startTime,
      error: error instanceof Error ? error.message : 'Unknown error',
      checkedAt,
    };
  }
}
```

**Step 2: Verify**

Run: `pnpm type-check`
Expected: PASS

**Step 3: Commit**

```bash
git add apps/admin/lib/ai/ollama-cloud.ts
git commit -m "feat(ai): add Ollama Cloud client with retry logic"
```

---

## Task 4: Create AI Module Index

**Files:**
- Create: `apps/admin/lib/ai/index.ts`

**Step 1: Create index file**

Create `apps/admin/lib/ai/index.ts`:

```typescript
/**
 * AI services module
 * - Ollama Cloud: LLM text generation (Llama 3.1 70B)
 * - Local Ollama: Embeddings (nomic-embed-text) - handled separately in search
 */

// Ollama Cloud client
export {
  isOllamaCloudConfigured,
  getConfiguredModel,
  listModels,
  generate,
  checkHealth,
} from './ollama-cloud';

// Types
export type {
  OllamaCloudConfig,
  OllamaGenerateRequest,
  OllamaGenerateResponse,
  OllamaHealthStatus,
  OllamaModelsResponse,
  AiResponse,
  AiResult,
  AiError,
} from './types';
```

**Step 2: Verify**

Run: `pnpm type-check`
Expected: PASS

**Step 3: Commit**

```bash
git add apps/admin/lib/ai/index.ts
git commit -m "feat(ai): add AI module exports"
```

---

## Task 5: Create Health Check API Route

**Files:**
- Create: `apps/admin/app/api/ai/health/route.ts`

**Step 1: Create health check route**

Create `apps/admin/app/api/ai/health/route.ts`:

```typescript
import { NextResponse } from 'next/server';

import { checkHealth, isOllamaCloudConfigured } from '@/lib/ai';
import { requireAuth } from '@/lib/auth';

/**
 * GET /api/ai/health - Check Ollama Cloud connection status
 * Returns health status including connectivity, model availability, and latency
 */
export async function GET() {
  // Require authentication to check AI health
  const session = await requireAuth();
  if (!session) {
    return NextResponse.json(
      { success: false, error: { code: 'UNAUTHORIZED', message: 'Unauthorized' } },
      { status: 401 }
    );
  }

  try {
    // Quick check if configured at all
    if (!isOllamaCloudConfigured()) {
      return NextResponse.json({
        success: true,
        data: {
          configured: false,
          connected: false,
          model: null,
          modelAvailable: false,
          latencyMs: null,
          error: 'OLLAMA_CLOUD_API_KEY not set',
          checkedAt: new Date().toISOString(),
        },
      });
    }

    // Full health check
    const health = await checkHealth();

    return NextResponse.json({
      success: true,
      data: {
        configured: true,
        ...health,
      },
    });
  } catch (error) {
    console.error('AI health check failed:', error);
    return NextResponse.json(
      {
        success: false,
        error: {
          code: 'SERVER_ERROR',
          message: 'Health check failed',
        },
      },
      { status: 500 }
    );
  }
}
```

**Step 2: Verify**

Run: `pnpm type-check`
Expected: PASS

**Step 3: Commit**

```bash
git add apps/admin/app/api/ai/health/route.ts
git commit -m "feat(ai): add health check API route"
```

---

## Task 6: Add Environment Variables Documentation

**Files:**
- Modify: `apps/admin/.env.example`

**Step 1: Add AI environment variables**

Add to `apps/admin/.env.example` (create if doesn't exist, or add to existing):

```bash
# =============================================================================
# AI Services - Ollama Cloud
# =============================================================================
# API key from Ollama Cloud Pro/Max subscription
# Get key from: https://ollama.com/settings/keys
# On VPS, copy from: /home/deploy/fiskai-intelligence/.env
OLLAMA_CLOUD_API_KEY=

# Ollama Cloud base URL (optional, defaults to https://api.ollama.com)
# OLLAMA_CLOUD_URL=https://api.ollama.com

# Model to use for text generation (optional, defaults to llama3.1:70b)
# OLLAMA_CLOUD_MODEL=llama3.1:70b

# =============================================================================
# AI Services - Local Ollama (Embeddings)
# =============================================================================
# Local Ollama instance on VPS for embeddings
# OLLAMA_LOCAL_URL=http://127.0.0.1:11434
```

**Step 2: Verify file exists**

Run: `ls -la apps/admin/.env.example`
Expected: File exists (or is newly created)

**Step 3: Commit**

```bash
git add apps/admin/.env.example
git commit -m "docs(ai): add Ollama Cloud environment variables to .env.example"
```

---

## Task 7: Add Unit Tests for Ollama Cloud Client

**Files:**
- Create: `apps/admin/lib/ai/__tests__/ollama-cloud.test.ts`

**Step 1: Create test file**

Create `apps/admin/lib/ai/__tests__/ollama-cloud.test.ts`:

```typescript
import { afterEach, beforeEach, describe, expect, it, vi } from 'vitest';

import {
  checkHealth,
  generate,
  getConfiguredModel,
  isOllamaCloudConfigured,
  listModels,
} from '../ollama-cloud';

// Mock fetch globally
const mockFetch = vi.fn();
global.fetch = mockFetch;

describe('ollama-cloud', () => {
  const originalEnv = process.env;

  beforeEach(() => {
    vi.resetAllMocks();
    process.env = {
      ...originalEnv,
      OLLAMA_CLOUD_API_KEY: 'test-api-key',
      OLLAMA_CLOUD_URL: 'https://api.ollama.com',
      OLLAMA_CLOUD_MODEL: 'llama3.1:70b',
    };
  });

  afterEach(() => {
    process.env = originalEnv;
  });

  describe('isOllamaCloudConfigured', () => {
    it('returns true when API key is set', () => {
      expect(isOllamaCloudConfigured()).toBe(true);
    });

    it('returns false when API key is not set', () => {
      delete process.env.OLLAMA_CLOUD_API_KEY;
      expect(isOllamaCloudConfigured()).toBe(false);
    });
  });

  describe('getConfiguredModel', () => {
    it('returns configured model', () => {
      expect(getConfiguredModel()).toBe('llama3.1:70b');
    });

    it('returns default model when not configured', () => {
      delete process.env.OLLAMA_CLOUD_MODEL;
      expect(getConfiguredModel()).toBe('llama3.1:70b');
    });
  });

  describe('listModels', () => {
    it('returns models on success', async () => {
      mockFetch.mockResolvedValueOnce({
        ok: true,
        status: 200,
        json: async () => ({
          models: [
            { name: 'llama3.1:70b', modified_at: '2024-01-01', size: 1000 },
          ],
        }),
      });

      const result = await listModels();

      expect(result.success).toBe(true);
      if (result.success) {
        expect(result.data.models).toHaveLength(1);
        expect(result.data.models[0].name).toBe('llama3.1:70b');
      }
    });

    it('returns AUTH_ERROR on 401', async () => {
      mockFetch.mockResolvedValueOnce({
        ok: false,
        status: 401,
      });

      const result = await listModels();

      expect(result.success).toBe(false);
      if (!result.success) {
        expect(result.error.code).toBe('AUTH_ERROR');
      }
    });

    it('returns RATE_LIMITED on 429', async () => {
      mockFetch.mockResolvedValueOnce({
        ok: false,
        status: 429,
        headers: new Headers({ 'Retry-After': '60' }),
      });

      const result = await listModels();

      expect(result.success).toBe(false);
      if (!result.success) {
        expect(result.error.code).toBe('RATE_LIMITED');
        expect(result.error.retryAfter).toBe(60);
      }
    });
  });

  describe('generate', () => {
    it('returns generated text on success', async () => {
      mockFetch.mockResolvedValueOnce({
        ok: true,
        status: 200,
        json: async () => ({
          model: 'llama3.1:70b',
          created_at: '2024-01-01T00:00:00Z',
          response: 'Generated text here',
          done: true,
          total_duration: 1000000000,
          prompt_eval_count: 10,
          eval_count: 20,
        }),
      });

      const result = await generate('Test prompt');

      expect(result.success).toBe(true);
      if (result.success) {
        expect(result.data.response).toBe('Generated text here');
        expect(result.data.done).toBe(true);
      }
    });

    it('includes system prompt when provided', async () => {
      mockFetch.mockResolvedValueOnce({
        ok: true,
        status: 200,
        json: async () => ({
          model: 'llama3.1:70b',
          response: 'Generated text',
          done: true,
        }),
      });

      await generate('Test prompt', { system: 'You are a helpful assistant' });

      expect(mockFetch).toHaveBeenCalledWith(
        expect.any(String),
        expect.objectContaining({
          body: expect.stringContaining('You are a helpful assistant'),
        })
      );
    });

    it('returns MODEL_NOT_FOUND on 404', async () => {
      mockFetch.mockResolvedValueOnce({
        ok: false,
        status: 404,
      });

      const result = await generate('Test prompt');

      expect(result.success).toBe(false);
      if (!result.success) {
        expect(result.error.code).toBe('MODEL_NOT_FOUND');
      }
    });

    it('does not retry on AUTH_ERROR', async () => {
      mockFetch.mockResolvedValue({
        ok: false,
        status: 401,
      });

      await generate('Test prompt');

      // Should only be called once (no retries for auth errors)
      expect(mockFetch).toHaveBeenCalledTimes(1);
    });
  });

  describe('checkHealth', () => {
    it('returns healthy status when connected', async () => {
      mockFetch.mockResolvedValueOnce({
        ok: true,
        status: 200,
        json: async () => ({
          models: [{ name: 'llama3.1:70b', modified_at: '2024-01-01', size: 1000 }],
        }),
      });

      const health = await checkHealth();

      expect(health.connected).toBe(true);
      expect(health.modelAvailable).toBe(true);
      expect(health.error).toBeNull();
      expect(health.latencyMs).toBeGreaterThanOrEqual(0);
    });

    it('returns not configured when API key missing', async () => {
      delete process.env.OLLAMA_CLOUD_API_KEY;

      const health = await checkHealth();

      expect(health.connected).toBe(false);
      expect(health.error).toContain('not configured');
    });

    it('reports model unavailable when not in list', async () => {
      mockFetch.mockResolvedValueOnce({
        ok: true,
        status: 200,
        json: async () => ({
          models: [{ name: 'other-model', modified_at: '2024-01-01', size: 1000 }],
        }),
      });

      const health = await checkHealth();

      expect(health.connected).toBe(true);
      expect(health.modelAvailable).toBe(false);
    });
  });
});
```

**Step 2: Run tests**

Run: `pnpm test apps/admin/lib/ai/__tests__/ollama-cloud.test.ts`
Expected: All tests PASS

**Step 3: Commit**

```bash
git add apps/admin/lib/ai/__tests__/ollama-cloud.test.ts
git commit -m "test(ai): add unit tests for Ollama Cloud client"
```

---

## Task 8: Final Verification

**Step 1: Run all checks**

```bash
cd /home/wandeon/WebVB
pnpm type-check
pnpm lint
pnpm test apps/admin/lib/ai/
pnpm build --filter=@repo/admin
```

Expected: All commands pass

**Step 2: Verify directory structure**

```bash
ls -la apps/admin/lib/ai/
```

Expected output:
```
total XX
drwxr-xr-x  index.ts
drwxr-xr-x  ollama-cloud.ts
drwxr-xr-x  types.ts
drwxr-xr-x  __tests__/
```

**Step 3: Manual test (when API is available)**

Once Ollama Cloud quota resets:

1. SSH to VPS: `ssh deploy@100.120.125.83`
2. Copy API key: `cat /home/deploy/fiskai-intelligence/.env | grep OLLAMA`
3. Add to admin `.env`: `OLLAMA_CLOUD_API_KEY=<key>`
4. Start admin app: `pnpm dev --filter=@repo/admin`
5. Test health endpoint: `curl http://localhost:3001/api/ai/health`
6. Expected response:
```json
{
  "success": true,
  "data": {
    "configured": true,
    "connected": true,
    "model": "llama3.1:70b",
    "modelAvailable": true,
    "latencyMs": 150,
    "error": null,
    "checkedAt": "2026-01-30T..."
  }
}
```

**Note:** Until quota resets, expect `connected: false` with rate limit error.

---

## Summary

| Task | Description |
|------|-------------|
| 1 | Add AI logger |
| 2 | Create AI types |
| 3 | Create Ollama Cloud client with retry logic |
| 4 | Create AI module index |
| 5 | Create health check API route |
| 6 | Add environment variables documentation |
| 7 | Add unit tests |
| 8 | Final verification |

## Files Created/Modified

**Created:**
- `apps/admin/lib/ai/types.ts` - AI service types
- `apps/admin/lib/ai/ollama-cloud.ts` - Ollama Cloud client
- `apps/admin/lib/ai/index.ts` - Module exports
- `apps/admin/app/api/ai/health/route.ts` - Health check endpoint
- `apps/admin/lib/ai/__tests__/ollama-cloud.test.ts` - Unit tests

**Modified:**
- `apps/admin/lib/logger.ts` - Add AI logger
- `apps/admin/.env.example` - Add Ollama Cloud variables

## Next Sprint

Sprint 6.2: AI Queue System - Worker that polls for pending jobs, processes with retry/backoff, updates status in real-time.
